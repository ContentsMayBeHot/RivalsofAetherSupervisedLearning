\chapter{Literature}
\label{ch:Literature}



\section{Literature Overview}
%-------------------------------------------%

In {\it Towards Integrated Imitation of Strategic Planning and Motion Modeling in Interactive Computer Games}, Dublin City University researchers Bernard Gorman and Mark Humphrys use imitation learning techniques to train an artificial agent to collect items in {\it Quake II} \cite{Gorman:2006}. Specifically, their agent must traverse three-dimensional environments in order to seek said items out without knowing their locations in advance. Furthermore, the researchers set an additional requirement: the agent must ``employ human-like motion,'' which is to say that it must traverse the map in such a way that its movements appear fluid and natural, as if a human were controlling it. The researchers refer to this ``imitation of the player's movement'' as {\it motion modeling}, and it represents the main purpose of their experiments. Gorman et al create their training dataset by retrieving state information from demos. Specifically, they use topology learning techniques to define a Markov Decision Process based on all of the player and pickup positions recorded in the demo file. They then employ value iteration using information about the player's status, such as current health and inventory, which is also read from the demo file. Value iteration eventually assigns a utility to each state in the MDP and, ultimately, allows the agent to make intelligent decisions about where to travel next. Gorman et al proceed to discuss their approach in solving the problem of motion modeling. They define a set of {\it action primitives} by reading all of the input data from the demo file and then disassociating that data from its context. This allows the agent to, after deciding where to navigate, lay out a series of action primitives that will allow it to reach its goal \cite{Gorman:2006}.

Gorman et al cite {\it Learning human-like Movement Behavior for Computer Games} by Christian Thurau and Christian Bauckhage of Bielefeld University, an experimental study which provides a useful behavioral model while also challenging the conventional approach of creating agents based on simple finite state machines \cite{Thurau:2004}. Thurau et al define a hierarchy of behavior types consisting of reactive, tactical, and strategic behaviors. Strategic behaviors are essentially long-term plans for winning the game, such as which objectives to target and in what order. Contrast this with reactive behaviors, which are atomic, immediate decisions, such as moving in a particular direction, aiming, or firing a weapon. Tactical behaviors sit in between reactive and strategic, representing small-scale decisions such as circle-strafing an opponent or choosing to run away from a battle in order to look for a health pack \cite{Gorman:2006}. To implement an agent that utilizes such behaviors, Thurau et al employ topology learning techniques to train a type of neural network called a {\it neural gas}, using player position data extracted from demo files. This methodology should sound familiar; Gorman et al use many of the contributions detailed in the paper \cite{Thurau:2004}.

In {\it Autoencoder-augmented Neuroevolution for Visual Doom Playing}, Cornell University researchers Samuel Alvernaz and Julian Togelius explore the use of an autoencoder alongside a neuroevolution technique called ``Covariance Matrix Adaptation Evolution Strategy'' or CMA-ES \cite{Alvernaz:2017}. An autoencoder is essentially a technique for simplifying and reconstructing images. The researchers implement their autoencoder using Keras, a high-level machine-learning library. They then use the autoencoder to generate a simplified version of the frame buffer to use as the input for their neural network, which they then train to complete health pack gathering exercises. The purpose of the autoencoder is to convert a high-dimensional state space, that being the full-resolution frame buffer, into a comparatively low-dimensional substitute \cite{Alvernaz:2017}.

In {\it Learning to Act by Predicting the Future}, Cornell University researchers Alexey Dosovitskiy and Vladlen Koltun use supervised learning techniques to train an agent to play competitive deathmatches in {\it Doom} \cite{Dosovitskiy:2016}. Dosovitskiy and Koltun do so by using two distinctive data streams: one is high-dimensional and consists of ``raw visual, auditory, and tactile input,'' while the other is low-dimensional and provides basic game state information such as ``health, ammunition levels, and the number of adversaries overcome'' (p. 1). These are referred to as the {\it sensory} and {\it measurement} streams, respectively \cite{Dosovitskiy:2016}.

In {\it Novel Moving Target Search Algorithms}, Peter K. K. Loch from Nanyang Technological University and Edmond C. Prakash from Manchester Metropolitan University explain the complexity behind creating an A.I. that can respond to a human player's actions both quickly and effectively \cite{Loh:2009}. They explain that this criteria creates conflicting requirements. They then continue to talk about two 'novel' MTS(Moving Target Search) Algorithms called Fuzzy MTS and Abstraction MTS. These algorithms were evaluated against existing MTS algorithms and were found to produce performance competitive to other MTS algorithms. This was shown to be the case even large problem spaces \cite{Loh:2009}.

In {\it Arnold: An Autonomous Agent to play FPS Games}, Devendra Singh Chaplot and Guillaume Lample from Carnegie Mellon University speak about their time creating a GameAgent for Doom and enter it into the popular competition ViZDoom. They speak briefly on their training methods for the Agent, mainly on the problems they faced during their time training the bot. They use UltraDeep and Theano as a base for their project and several other common machine learning techniques to overcome many of these problems. Their result was a bot with the highest K/D(Kill/Death) ratio in the tournament \cite{Chaplot}.

In {\it ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement}, Cornell University researchers Michal Kempa, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Ja≈õkowski discuss the benefits of ViZDoom as a research platform \cite{Kempka:2016}. They first discuss the projects that led to the creation of ViZDoom, those being Atari 2600 A.I. projects. They discuss the jump from 2D to 3D environments. While it may seem like a large jump in reality, a 3D space can be abstracted into many 2D environments. They go on to describe ViZDoom as a lightweight, fast and highly customizable framework. They researchers designed two bots and trained them. The result was two human-like A.I. which proved ViZDoom to be a useful project that could really contribute to the field \cite{Kempka:2016}.

In {\it MariFlow - Self-Driving Mario Kart w/Recurrent Neural Network}, YouTuber SethBling trains a recurrent neural network to play Mario Kart using only the visual buffer \cite{SethBling:2017}. The goal of SethBling's project is to train the neural network to mimic the way he plays the game by having it learn from fifteen hours of his own recorded gameplay. The network, created in TensorFlow, receives "a low-resolution grayscale version of the screen" for input and employs multiple layers, sigmoid neurons, backpropagation, and "long short term memory" or LSTM cells in order to predict the appropriate controller action. Furthermore, SethBling runs "interactive sessions" in which he monitors the neural network while it plays the game so that he can identify situations in which it gets stuck; he then takes over control of the game so that he can show the neural network what to do in such situations. SethBling released several learning resources with his video, including his source code, utilities, and detailed instructions \cite{SethBling:2017}. Overall, not only does SethBlin provide an excellent entry-level primer for the fundamentals of neural networks, he also gives an encouraging demonstration on how to solve a problem that is very close to our own.

\section{Software}
%-------------------------------------------%
{\it SerpentAI} is an open source Python framework that allows researchers to write scripts that can easily retrieve the frame buffer from a game and then send keyboard or controller input back to it. The framework is noteworthy for functioning entirely independently of the target game. Furthermore, it makes no presumptions about what machine learning techniques the user wants to implement and can support anything from simple reflex or random agents to the most advanced and cutting-edge neural networks.

The target game for this project is {\it Rivals of Aether}, a 2D fighting game created by independent game developer Dan Fornace. We selected {\it Rivals of Aether} because it supports the recording of replays, which are files that keep a record of what controller inputs were detected from which player during each frame of gameplay. Thus, whereas a conventional video recording associates each frame with its pixel data, a replay does so with each frame and its input data. This allows the game to simulate a past match in real-time with perfect accuracy by simply running a new match in which each player's control input is read from the replay file rather than from the controller.

\begin{figure}
	\caption{"Screenshot showing gameplay in Rivals of Aether"}
	\centering
	\includegraphics[scale = 0.5]{game.png} \\
\end{figure}

A replay file is therefore no different from a demo file from {\it Quake} or {\it Doom}. Such demos are often used in game-related machine learning research because a collection of demos is effectively a pre-labeled dataset. However, unlike {\it Doom} and {\it Quake}, which have websites such as {\it Speed Demo Archives} and {\it Quake Terminus}, {\it Rivals of Aether} does not have any well-established public repositories for replay files. Therefore, in order to build our collection of replays, we solicited from the {\it Rivals of Aether} community. Contributors provided us with a total of 1,020 replay files from a variety of settings, including tournaments, exhibitions, ranked matches, and private games. Adding our own personal replays brings the total up to 1,032. 

%-------------------------------------------%



\section{Other Sources}
%-------------------------------------------%



%-------------------------------------------%