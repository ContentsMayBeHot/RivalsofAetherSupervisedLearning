\chapter{Literature}
\label{ch:Literature}



\section{Literature Overview}
%-------------------------------------------%

In {\it Towards Integrated Imitation of Strategic Planning and Motion Modeling in Interactive Computer Games}, Dublin City University researchers Bernard Gorman and Mark Humphrys use imitation learning techniques to train an artificial agent to collect items in {\it Quake II} \cite{Gorman:2006}. Specifically, their agent must traverse three-dimensional environments in order to seek said items out without knowing their locations in advance. Furthermore, the researchers set an additional requirement: the agent must ``employ human-like motion,'' which is to say that it must traverse the map in such a way that its movements appear fluid and natural, as if a human were controlling it. The researchers refer to this ``imitation of the player's movement'' as {\it motion modeling}, and it represents the main purpose of their experiments. Gorman and Humphrys create their training dataset by retrieving state information from demos. Specifically, they use topology learning techniques to define a Markov Decision Process based on all of the player and pickup positions recorded in the demo file. They then employ value iteration using information about the player's status, such as current health and inventory, which is also read from the demo file. Value iteration eventually assigns a utility to each state in the MDP and, ultimately, allows the agent to make intelligent decisions about where to travel next. Gorman and Humphrys proceed to discuss their approach in solving the problem of motion modeling. They define a set of {\it action primitives} by reading all of the input data from the demo file and then disassociating that data from its context. This allows the agent to, after deciding where to navigate, lay out a series of action primitives that will allow it to reach its goal \cite{Gorman:2006}.

Gorman and Humphrys cite {\it Learning human-like Movement Behavior for Computer Games} by Christian Thurau and Christian Bauckhage of Bielefeld University, an experimental study which provides a useful behavioral model while also challenging the conventional approach of creating agents based on simple finite state machines \cite{Thurau:2004}. Thurau et al define a hierarchy of behavior types consisting of reactive, tactical, and strategic behaviors. Strategic behaviors are essentially long-term plans for winning the game, such as which objectives to target and in what order. Contrast this with reactive behaviors, which are atomic, immediate decisions, such as moving in a particular direction, aiming, or firing a weapon. Tactical behaviors sit in between reactive and strategic, representing small-scale decisions such as circle-strafing an opponent or choosing to run away from a battle in order to look for a health pack \cite{Gorman:2006}. To implement an agent that utilizes such behaviors, Thurau and Bauckhage employ topology learning techniques to train a type of neural network called a {\it neural gas}, using player position data extracted from demo files. This methodology should sound familiar; Gorman and Humphrys use many of the contributions detailed in the paper \cite{Thurau:2004}.

The research described heretofore addresses considerably more complex problems by virtue of their game environment being three rather than two-dimensional. Nonetheless, the problems are very similar, and as such the solutions may also bear a resemblance. Thurau and Bauckhage demonstrate the use of topological learning with a neural gas \cite{Thurau:2004}. Gorman and Humphrys initially take a similar approach, using topological learning as well, but then proceed to solve their problem by defining it as an MDP \cite{Gorman:2006}. We should consider the possibility of translating these techniques into a two-dimensional state space.

In {\it Learning to Act by Predicting the Future}, Cornell University researchers Alexey Dosovitskiy and Vladlen Koltun use supervised learning techniques to train an agent to play competitive deathmatches in {\it Doom} \cite{VDComp:2016}. Dosovitskiy and Koltun do so by using two distinctive data streams: one is high-dimensional and consists of ``raw visual, auditory, and tactile input,'' while the other is low-dimensional and provides basic game state information such as ``health, ammunition levels, and the number of adversaries overcome'' (p. 1). These are referred to as the {\it sensory} and {\it measurement} streams, respectively, and they serve as inputs for a supervised learning model \cite{Dosovitskiy:2016}. The resulting AI, named IntelAct, performed remarkably well in the full deathmatch track of 2016 Visual Doom AI Competition \cite{VDComp:2016}.

However, on the same track in the 2017 competition, IntelAct was beaten by two other AI, with YanShi and Arnold4 coming in second and first place, respectively \cite{VDComp:2017}. In {\it Arnold: An Autonomous Agent to play FPS Games}, Devendra Singh Chaplot and Guillaume Lample from Carnegie Mellon University use a pair of Deep Q-Networks to train an AI to play Doom. One of the networks is concerned with learning policies related to combat while the other deals with environmental navigation. This model is implemented with UltraDeep and Theano. The result is an AI that ``outperforms average humans as well as in-built game bots'' and that earned ``the highest kill-to-death ratio in both tracks of the Visual Doom AI Competition.'' Overall, this paper provides a convenient overview of the project \cite{Chaplot}. However, Chaplot and Lample go into far greater detail in the paper titled {\it Playing FPS Games with Deep Reinforcement Learning} \cite{Chaplot:2016}.

The results of Arnold and IntelAct deserve much attention because they not only implemented creative solutions but they also outperformed all of the other AIs during the 2017 and 2016 competitions, respectively. One notable common thread between the two projects is that the researchers found ways to break the problem into smaller, more manageable pieces; Chaplot and Lample do this at the learning stage by using two deep reinforcement learning networks \cite{Chaplot}, while Dosovitskiy and Koltun work from the input stage with their dual-stream approach \cite{Dosovitskiy:2016}.

In {\it Novel Moving Target Search Algorithms}, Peter K. K. Loh from Nanyang Technological University and Edmond C. Prakash from Manchester Metropolitan University explain the complexity behind creating an AI that can respond to a human player's actions both quickly and effectively \cite{Loh:2009}. They explain that this criteria creates conflicting requirements. They then continue to talk about two `novel' MTS(Moving Target Search) Algorithms called Fuzzy MTS and Abstraction MTS. These algorithms were evaluated against existing MTS algorithms and were found to produce performance competitive to other MTS algorithms. This was shown to be the case even large problem spaces \cite{Loh:2009}.

In {\it ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement}, Cornell University researchers Michal Kempa, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Ja≈õkowski discuss the benefits of ViZDoom as a research platform \cite{Kempka:2016}. They first discuss the projects that led to the creation of ViZDoom, those being Atari 2600 A.I. projects. They discuss the jump from 2D to 3D environments. While it may seem like a large jump in reality, a 3D space can be abstracted into many 2D environments. They go on to describe ViZDoom as a lightweight, fast and highly customizable framework. They researchers designed two bots and trained them. The result was two human-like A.I. which proved ViZDoom to be a useful project that could really contribute to the field \cite{Kempka:2016}.

In {\it Autoencoder-augmented Neuroevolution for Visual Doom Playing}, Cornell University researchers Samuel Alvernaz and Julian Togelius explore the use of an autoencoder alongside a neuroevolution technique called ``Covariance Matrix Adaptation Evolution Strategy'' or CMA-ES \cite{Alvernaz:2017}. An autoencoder is essentially a technique for simplifying and reconstructing images. The researchers implement their autoencoder using Keras, a high-level machine-learning library. They then use the autoencoder to generate a simplified version of the frame buffer to use as the input for their neural network, which they then train to complete health pack gathering exercises. The autoencoder's purpose is to convert a high-dimensional state space, that being the full-resolution frame buffer, into a comparatively low-dimensional substitute \cite{Alvernaz:2017}. The use of an autoencoder by Alvernaz et al can be directly compared with SethBling's use of simple downscaling and grayscaling. Both of these techniques result in a simplified state space; however, the key difference between them is that an autoencoder has the potential to learn a much more efficient way to achieve this goal. Otherwise, the one other key technique noted in this paper is CMA-ES. If our dataset, for whatever reason, proves insufficient or otherwise unusable, then neuroevolution algorithms such as this may be our best fall-back option.

\section{Software}
%-------------------------------------------%

In {\it MariFlow - Self-Driving Mario Kart w/Recurrent Neural Network}, YouTuber SethBling trains a recurrent neural network to play Mario Kart using only the visual buffer \cite{SethBling:2017}. The goal of SethBling's project is to train the neural network to mimic the way he plays the game by having it learn from fifteen hours of his own recorded gameplay. The network, created in TensorFlow, receives ``a low-resolution grayscale version of the screen'' for input and employs multiple layers, sigmoid neurons, backpropagation, and ``long short term memory'' or LSTM cells in order to predict the appropriate controller action. Furthermore, SethBling runs ``interactive sessions'' in which he monitors the neural network while it plays the game so that he can identify situations in which it gets stuck; he then takes over control of the game so that he can show the neural network what to do in such situations. SethBling released several learning resources with his video, including his source code, utilities, and detailed instructions \cite{SethBling:2017}. Overall, not only does SethBling provide an excellent entry-level primer for the fundamentals of neural networks, he also gives an encouraging demonstration on how to solve a problem that is very close to our own.

%-------------------------------------------%



\section{Other Sources}
%-------------------------------------------%



%-------------------------------------------%